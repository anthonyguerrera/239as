{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"JdmpKitw8XHp"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1094,"status":"ok","timestamp":1717869981507,"user":{"displayName":"Anthony Guerrera","userId":"15652414766004964920"},"user_tz":240},"id":"n2oDK4_38kts","outputId":"cc20a2db-ddc7-44a9-bd2c-c393ac50b9a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Lab4\n"]}],"source":["# anthony\n","from google.colab import drive\n","drive.mount('/content/drive')\n","# %ls\n","%cd /content/drive/MyDrive/Lab4\n","\n","# Gavin\n","# %cd /content/drive/MyDrive/Undergraduate/Junior/Spring 2024/ECE 239 AS - Labs/Lab 4 - RLp2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":310,"status":"ok","timestamp":1717869984849,"user":{"displayName":"Anthony Guerrera","userId":"15652414766004964920"},"user_tz":240},"id":"nQNlG7wRdQQt","outputId":"32ce8820-66a1-4cfb-f009-b42e167e129a"},"outputs":[{"output_type":"stream","name":"stdout","text":["'Anthony Copy of rl.ipynb'   env_wrapper.py     \u001b[0m\u001b[01;34mruns\u001b[0m/\n","'Copy of rl.ipynb'           model.py           test_outputs.pt\n"," DDPG.png                    \u001b[01;34m__pycache__\u001b[0m/       test_replay_buffer_inputs.pkl\n"," DQN.png                     replay_buffer.py   test_replay_buffer_samples.pth\n"," DQN.py                      rl.ipynb           test_weights.pt\n","'env_wrapper[bad].py'        rl_test.py         utils.py\n"]}],"source":["%ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":147173,"status":"ok","timestamp":1717870133435,"user":{"displayName":"Anthony Guerrera","userId":"15652414766004964920"},"user_tz":240},"id":"OF-2K6o88XHt","outputId":"c9bf1db3-f311-430c-b152-3b97442d538a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n","Collecting wandb\n","  Downloading wandb-0.17.1-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (2.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Collecting docker-pycreds>=0.4.0 (from wandb)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n","  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n","Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n","Collecting sentry-sdk>=1.0.0 (from wandb)\n","  Downloading sentry_sdk-2.5.1-py2.py3-none-any.whl (289 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.6/289.6 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting setproctitle (from wandb)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.6.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Installing collected packages: smmap, setproctitle, sentry-sdk, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, docker-pycreds, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gitdb, nvidia-cusolver-cu12, gitpython, wandb\n","Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 sentry-sdk-2.5.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.1\n","Collecting gymnasium\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.1)\n","Collecting farama-notifications>=0.0.1 (from gymnasium)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, gymnasium\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n","Collecting swig\n","  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: swig\n","Successfully installed swig-4.2.1\n","Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.1)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n","Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n","  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n","Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.2.1)\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2376133 sha256=30fff117042381ff5b4952f1f731d16b4448da010978c7dfacf55278410d5e2b\n","  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n","Successfully built box2d-py\n","Installing collected packages: box2d-py\n","Successfully installed box2d-py-2.3.5\n"]}],"source":["import traceback\n","!pip install numpy torch wandb matplotlib termcolor\n","!pip install gymnasium\n","!pip install swig\n","!pip install gymnasium[box2d]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y9XCQjmZ8XHu"},"outputs":[],"source":["import test\n","from utils import *\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3809,"status":"ok","timestamp":1717870214212,"user":{"displayName":"Anthony Guerrera","userId":"15652414766004964920"},"user_tz":240},"id":"WcGjNZdkexND","outputId":"6de91853-4662-4823-af1f-1d4de692e2ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}],"source":["# Gavin\n","import importlib.util\n","module_name = 'rl_test'\n","# module_path = '/content/drive/MyDrive/Undergraduate/Junior/Spring 2024/ECE 239 AS - Labs/Lab 4 - RLp2/rl_test.py'\n","module_path = '/content/drive/MyDrive/Lab4/rl_test.py'\n","\n","spec = importlib.util.spec_from_file_location(module_name, module_path)\n","rl_test = importlib.util.module_from_spec(spec)\n","spec.loader.exec_module(rl_test)\n","\n","# Now test if the import was successful by accessing a function or attribute\n","print(hasattr(rl_test, 'test_wrapper'))  # replace with an actual function or variable name from rl_test"]},{"cell_type":"markdown","metadata":{"id":"gM2DOuGy8XHv"},"source":["# Reinforcement Learning Part 1: DQN\n","By Lawrence Liu and Tonmoy Monsoor\n","## Some General Instructions\n","\n","- As before, please keep the names of the layer consistent with what is requested in model.py. Otherwise the test functions will not work\n","\n","- You will need to fill in the model.py, the DQN.py file, the buffer.py file, and the\n","env_wrapper.py\n","\n","DO NOT use Windows for this project, gymnasium does is not supported for windows and installing it will be a pain."]},{"cell_type":"markdown","metadata":{"id":"Q3ryIeN38XHx"},"source":["### Introduction to the Enviroment\n","We will be training a DQN agent to play the game of CarRacing. The agent will be trained to play the game using the pixels of the game as an input. The reward structure is as follows for each frame:\n","- -0.1 for each frame\n","- +1000/N where N is the number of tiles visited by the car in the episode\n","\n","The overall goal of this game is to design a agent that is able to play the game with a average test score of above 600. In discrete mode the actions can take 5 actions,\n","- 0: Do Nothing\n","- 1: Turn Left\n","- 2: Turn Right\n","- 3: Accelerate\n","- 4: Brake\n","\n","First let us visualize the game and understand the environment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PT5Sj8M08XHx"},"outputs":[],"source":["import gymnasium as gym\n","import numpy as np\n","env = gym.make('CarRacing-v2', continuous=False, render_mode='rgb_array')\n","env.np_random = np.random.RandomState(42)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"output_embedded_package_id":"1KNisjhq0ZgHpZLFJqoNu6eZ7-stQECRn","base_uri":"https://localhost:8080/","height":1000},"id":"JiTcrSOu8XHy","outputId":"6eb2e552-a23d-42aa-aae4-8a6985e56e04","executionInfo":{"status":"ok","timestamp":1717870258994,"user_tz":240,"elapsed":38608,"user":{"displayName":"Anthony Guerrera","userId":"15652414766004964920"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["\n","from IPython.display import HTML\n","\n","frames = []\n","s, _ = env.reset()\n","\n","while True:\n","    a = env.action_space.sample()\n","    s, r, terminated, truncated, _ = env.step(a)\n","    frames.append(s)\n","    if terminated or truncated:\n","        break\n","\n","\n","anim = animate(frames)\n","HTML(anim.to_jshtml())"]},{"cell_type":"markdown","metadata":{"id":"CrdCFYtr8XHy"},"source":["So a couple things we can note:\n","- at the beginning of the game, we have 50 frames of the game slowly zooming into the car, we should ignore this period, ie no-op during this period.\n","- there is a black bar at the bottom of the screen, we should crop this out of the observation.\n","\n","In addition, another thing to note is that the current frame doesn't give much information about the velocity and acceleration of the car, and that the car does not move much for each frame.\n","### Environment Wrapper (5 points)\n","As a result, you will need to complete `EnvWrapper` in `env_wrapper.py`. You can find more information in the docstring for the wrapper, however the main idea is that it is a wrapper to the environment that does the following:\n","- skips the first 50 frames of the game\n","- crops out the black bar and reshapes the observation to a 84x84 image, as well as turning the resulting image to grayscale\n","- performs the actions for `skip_frames` frames\n","- stacks the last `num_frames` frames together to give the agent some information about the velocity and acceleration of the car.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12151,"status":"ok","timestamp":1717870275869,"user":{"displayName":"Anthony Guerrera","userId":"15652414766004964920"},"user_tz":240},"id":"OczKr9lv8XHz","outputId":"f13ff970-9eaa-42ce-a031-9e6c8c791062"},"outputs":[{"output_type":"stream","name":"stdout","text":["hi\n","Passed reset\n","Passed step\n"]}],"source":["from env_wrapper import EnvWrapper\n","\n","rl_test.test_wrapper(EnvWrapper)\n","# test.test_wrapper(EnvWrapper) #old"]},{"cell_type":"markdown","metadata":{"id":"TzdXa1zV8XHz"},"source":["### CNN Model (5 points)\n","Now we are ready to build the model. Our architecture of the CNN model is the one proposed by Mnih et al in \"Human-level control through deep reinforcement learning\". Specifically this consists of the following layers:\n","- A convolutional layer with 32 filters of size 8x8 with stride 4 and relu activation\n","- A convolutional layer with 64 filters of size 4x4 with stride 2 and relu activation\n","- A convolutional layer with 64 filters of size 3x3 with stride 1 and relu activation\n","- A fully connected layer with 512 units and relu activation\n","- A fully connected layer with the number of outputs of the environment\n","\n","Please implement this model `Nature_Paper_Conv` in `model.py` as well as the helper\n","`MLP` class."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2003,"status":"ok","timestamp":1717870279635,"user":{"displayName":"Anthony Guerrera","userId":"15652414766004964920"},"user_tz":240},"id":"KBeIzXzL8XH0","outputId":"7464ae12-169d-4b2d-cd95-9c7d2c96a789"},"outputs":[{"output_type":"stream","name":"stdout","text":["Passed\n"]}],"source":["import model\n","\n","rl_test.test_model_DQN(model.Nature_Paper_Conv)\n","# test.test_model_DQN(model.Nature_Paper_Conv)old"]},{"cell_type":"markdown","metadata":{"id":"llqfzCMS8XH0"},"source":["### DQN (40 points)\n","Now we are ready to implement the DQN algorithm.\n","\n","![title](DQN.png)\n","\n","#### Replay Buffer (5 points)\n","First start by implementing the DQN replay buffer `ReplayBufferDQN` in `buffer.py`. This buffer will store the transitions of the agent and sample them for training."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3779,"status":"ok","timestamp":1717870285151,"user":{"displayName":"Anthony Guerrera","userId":"15652414766004964920"},"user_tz":240},"id":"LQi-qju18XH1","outputId":"2ba502c2-b752-45a6-b658-0de4b788060e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Passed\n"]}],"source":["from replay_buffer import ReplayBufferDQN\n","\n","rl_test.test_DQN_replay_buffer(ReplayBufferDQN)"]},{"cell_type":"markdown","metadata":{"id":"PSjHyiJF8XH1"},"source":["\n","#### DQN (15 points)\n","Now implement the `_optimize_model` and `sample_action` functions in `DQN` in `DQN.py`. The `_optimize_model` function will sample a batch of transitions from the replay buffer and update the model. The `sample_action` function will sample an action from the model given the current state. Train the model over 200 episdoes, validating every 50 episodes for 30 episodes, before testing the model for 50 episodes at the end."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0E6l0vZW8XH1","outputId":"3d2aa2b5-cceb-4658-8249-ede451516d73"},"outputs":[{"output_type":"stream","name":"stdout","text":["saving to ./runs/DQN/run7\n","Episode: 0: Time: 9.516499280929565 Total Reward: -40.51282051282095 Avg_Loss: 0\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/Lab4/DQN.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  states = torch.tensor(states, dtype=torch.float32).to(self.device)\n","/content/drive/MyDrive/Lab4/DQN.py:155: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1).to(self.device)\n","/content/drive/MyDrive/Lab4/DQN.py:156: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(self.device)\n","/content/drive/MyDrive/Lab4/DQN.py:157: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n","/content/drive/MyDrive/Lab4/DQN.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(self.device)\n"]},{"output_type":"stream","name":"stdout","text":["Episode: 1: Time: 27.150578260421753 Total Reward: -18.357664233576525 Avg_Loss: 124.18194939941168\n","Episode: 2: Time: 32.92455172538757 Total Reward: -73.57142857142878 Avg_Loss: 199.401286220178\n","Episode: 3: Time: 32.09942603111267 Total Reward: -64.69696969697027 Avg_Loss: 175.82508347183466\n","Episode: 4: Time: 35.145201683044434 Total Reward: -77.39436619718305 Avg_Loss: 194.42427204176784\n","Episode: 5: Time: 34.209285259246826 Total Reward: -75.91603053435125 Avg_Loss: 141.82517356798053\n","Episode: 6: Time: 35.22795271873474 Total Reward: -36.40625000000064 Avg_Loss: 134.416475340724\n","Episode: 7: Time: 36.694637298583984 Total Reward: -37.02898550724653 Avg_Loss: 158.89870229363441\n","Episode: 8: Time: 37.24000310897827 Total Reward: -23.03030303030359 Avg_Loss: 192.31574001908302\n","Episode: 9: Time: 34.55461835861206 Total Reward: -32.908496732026585 Avg_Loss: 185.83714767545462\n","Episode: 10: Time: 36.49099946022034 Total Reward: 74.67509025270715 Avg_Loss: 232.94497452676296\n","Episode: 11: Time: 35.97851490974426 Total Reward: -61.76737160120911 Avg_Loss: 1084.7712201923132\n","Episode: 12: Time: 37.444350242614746 Total Reward: -33.5665529010246 Avg_Loss: 235.79863379895687\n","Episode: 13: Time: 35.46492290496826 Total Reward: -27.203389830509185 Avg_Loss: 277.9011594951153\n","Episode: 14: Time: 35.170923948287964 Total Reward: 17.540192926046025 Avg_Loss: 366.50555624067783\n","Episode: 15: Time: 37.01598262786865 Total Reward: 410.2631578947352 Avg_Loss: 505.9642450809479\n","Episode: 16: Time: 37.24131369590759 Total Reward: -30.5932203389838 Avg_Loss: 969.9645328521729\n","Episode: 17: Time: 38.971251010894775 Total Reward: 66.99376947040899 Avg_Loss: 931.9727566838264\n","Episode: 18: Time: 38.007851362228394 Total Reward: 186.55339805825537 Avg_Loss: 1441.1017437577248\n","Episode: 19: Time: 35.87441849708557 Total Reward: 59.9295774647886 Avg_Loss: 1490.1344398260117\n","Episode: 20: Time: 33.767884492874146 Total Reward: 13.108108108109 Avg_Loss: 1682.2669039964676\n","Episode: 21: Time: 35.446653842926025 Total Reward: 25.567375886526435 Avg_Loss: 1754.9547159671783\n","Episode: 22: Time: 34.51395034790039 Total Reward: 166.6487455197165 Avg_Loss: 1446.1525927782059\n","Episode: 23: Time: 35.54976010322571 Total Reward: -46.92307692307738 Avg_Loss: 2769.3973129987717\n","Episode: 24: Time: 36.94349026679993 Total Reward: 201.70329670330102 Avg_Loss: 2007.114036142826\n","Episode: 25: Time: 37.59538793563843 Total Reward: 251.15384615384986 Avg_Loss: 1828.415552675724\n","Episode: 26: Time: 39.25159239768982 Total Reward: -32.06293706293769 Avg_Loss: 1706.1888962388039\n","Episode: 27: Time: 37.99260687828064 Total Reward: 49.200626959249476 Avg_Loss: 2043.6518787145615\n","Episode: 28: Time: 34.077016830444336 Total Reward: -38.6619718309865 Avg_Loss: 2020.4555395841599\n","Episode: 29: Time: 36.92895793914795 Total Reward: 263.42293906809834 Avg_Loss: 2019.9247469902039\n","Episode: 30: Time: 35.518691062927246 Total Reward: 70.62500000000134 Avg_Loss: 2263.9041303396225\n","Episode: 31: Time: 39.52169680595398 Total Reward: 34.277566539923086 Avg_Loss: 1767.7376859784126\n","Episode: 32: Time: 37.51249432563782 Total Reward: 146.83006535948135 Avg_Loss: 2450.567319750786\n","Episode: 33: Time: 35.39253377914429 Total Reward: -3.227848101266706 Avg_Loss: 2624.3426785469055\n","Episode: 34: Time: 37.533751249313354 Total Reward: 2.4842767295589536 Avg_Loss: 1946.9515351057053\n","Episode: 35: Time: 38.56589937210083 Total Reward: 64.46843853820545 Avg_Loss: 2577.293335556984\n","Episode: 36: Time: 39.75706434249878 Total Reward: 63.87850467289877 Avg_Loss: 2273.490201115608\n","Episode: 37: Time: 37.04896545410156 Total Reward: -68.8805970149258 Avg_Loss: 2019.7530317306519\n","Episode: 38: Time: 38.10450792312622 Total Reward: 53.73417721518915 Avg_Loss: 1958.1908539533615\n","Episode: 39: Time: 36.288612604141235 Total Reward: 28.636363636362304 Avg_Loss: 2169.5398864746094\n","Episode: 40: Time: 35.43499732017517 Total Reward: -53.01526717557311 Avg_Loss: 2020.9745002985\n","Episode: 41: Time: 40.26906418800354 Total Reward: 151.7105263157934 Avg_Loss: 2285.249320745468\n","Episode: 42: Time: 37.310933113098145 Total Reward: 28.636363636365765 Avg_Loss: 2667.2270673513412\n","Episode: 43: Time: 36.07093334197998 Total Reward: 220.25423728813877 Avg_Loss: 2320.8987305164337\n","Episode: 44: Time: 36.966787338256836 Total Reward: -19.6575342465757 Avg_Loss: 2853.5862456560135\n","Episode: 45: Time: 37.12973499298096 Total Reward: 183.57142857143265 Avg_Loss: 1282.3937315940857\n","Episode: 46: Time: 35.86998176574707 Total Reward: 135.0319488817937 Avg_Loss: 3377.008693933487\n","Episode: 47: Time: 36.50137543678284 Total Reward: 428.8095238095158 Avg_Loss: 1950.4835340976715\n","Episode: 48: Time: 35.97364568710327 Total Reward: 316.56462585033887 Avg_Loss: 2656.21941447258\n","Episode: 49: Time: 37.67112851142883 Total Reward: 32.27272727272688 Avg_Loss: 3381.409001350403\n","Validation Mean Reward: 42.65570744197281 Validation Std Reward: 150.2859069976659\n","Episode: 50: Time: 38.89153480529785 Total Reward: -27.721712538226985 Avg_Loss: 2983.284000992775\n","Episode: 51: Time: 34.20911145210266 Total Reward: 122.68707482993288 Avg_Loss: 2930.8382402658463\n","Episode: 52: Time: 37.992663621902466 Total Reward: 84.21146953405352 Avg_Loss: 3861.092792868614\n","Episode: 53: Time: 36.26148438453674 Total Reward: 210.14705882353286 Avg_Loss: 5036.1223702430725\n","Episode: 54: Time: 36.2829864025116 Total Reward: 282.69784172660945 Avg_Loss: 2677.5445914268494\n","Episode: 55: Time: 36.27776575088501 Total Reward: 192.44939271255484 Avg_Loss: 2911.2001202106476\n","Episode: 56: Time: 34.69503879547119 Total Reward: 293.69257950529214 Avg_Loss: 2733.7062162160873\n","Episode: 57: Time: 26.346975088119507 Total Reward: -9.855234657036902 Avg_Loss: 2366.1598858833313\n","Episode: 58: Time: 35.97534918785095 Total Reward: -1.2499999999998601 Avg_Loss: 3288.483011841774\n","Episode: 59: Time: 33.65915656089783 Total Reward: 150.42124542124975 Avg_Loss: 6132.226059794426\n","Episode: 60: Time: 37.34072756767273 Total Reward: 214.67741935484176 Avg_Loss: 4716.207943677902\n","Episode: 61: Time: 35.46676731109619 Total Reward: 115.00000000000009 Avg_Loss: 3825.031967282295\n","Episode: 62: Time: 36.758986949920654 Total Reward: 159.41696113074545 Avg_Loss: 2713.3474158644676\n","Episode: 63: Time: 35.33230257034302 Total Reward: 119.7651006711431 Avg_Loss: 4719.639261007309\n","Episode: 64: Time: 37.58538341522217 Total Reward: 180.31645569620662 Avg_Loss: 3649.0330057144165\n","Episode: 65: Time: 37.085843563079834 Total Reward: 172.80626780626818 Avg_Loss: 3898.1979674100876\n","Episode: 66: Time: 35.064032793045044 Total Reward: 149.06779661017373 Avg_Loss: 4064.6145817041397\n","Episode: 67: Time: 35.284470081329346 Total Reward: 164.7864768683317 Avg_Loss: 3364.814761161804\n","Episode: 68: Time: 33.88361310958862 Total Reward: 268.2653061224446 Avg_Loss: 3877.5205825567245\n","Episode: 69: Time: 36.509533166885376 Total Reward: 133.13688212928244 Avg_Loss: 5162.678541898727\n","Episode: 70: Time: 34.74619102478027 Total Reward: 274.5652173913021 Avg_Loss: 4895.6100997924805\n","Episode: 71: Time: 35.82614827156067 Total Reward: 24.122257053290625 Avg_Loss: 6956.280046820641\n","Episode: 72: Time: 35.46968388557434 Total Reward: 364.8070739549806 Avg_Loss: 3071.6839578151703\n","Episode: 73: Time: 35.45303821563721 Total Reward: 259.166666666669 Avg_Loss: 4005.1609773635864\n","Episode: 74: Time: 38.644752502441406 Total Reward: 129.91349480968836 Avg_Loss: 5338.044909000397\n","Episode: 75: Time: 37.88704490661621 Total Reward: 214.37500000000395 Avg_Loss: 4100.272589206696\n","Episode: 76: Time: 36.477936029434204 Total Reward: 173.85245901639792 Avg_Loss: 3506.867087483406\n","Episode: 77: Time: 35.54870271682739 Total Reward: 160.17241379310641 Avg_Loss: 6044.805405139923\n","Episode: 78: Time: 39.462015867233276 Total Reward: 98.33333333333333 Avg_Loss: 4700.486283540726\n","Episode: 79: Time: 37.32903456687927 Total Reward: 105.00000000000097 Avg_Loss: 5659.208317518234\n","Episode: 80: Time: 37.14526152610779 Total Reward: 289.61538461537447 Avg_Loss: 4010.2510352134705\n","Episode: 81: Time: 36.63330698013306 Total Reward: 95.14084507042257 Avg_Loss: 5720.02059674263\n","Episode: 82: Time: 37.30068254470825 Total Reward: 198.33333333333672 Avg_Loss: 4434.223713159561\n","Episode: 83: Time: 39.210413694381714 Total Reward: 183.93175074184137 Avg_Loss: 4072.9239871501923\n","Episode: 84: Time: 38.49148178100586 Total Reward: 272.6470588235332 Avg_Loss: 4363.475740909576\n","Episode: 85: Time: 35.653576612472534 Total Reward: 185.13029315960932 Avg_Loss: 4406.9926788806915\n","Episode: 86: Time: 37.187381744384766 Total Reward: 213.7719298245624 Avg_Loss: 4988.219905376434\n","Episode: 87: Time: 39.73563289642334 Total Reward: -36.55844155844221 Avg_Loss: 3440.995440006256\n","Episode: 88: Time: 37.556002378463745 Total Reward: 175.67669172932744 Avg_Loss: 4378.767527103424\n","Episode: 89: Time: 36.99217987060547 Total Reward: 227.88401253918823 Avg_Loss: 4629.376670598984\n","Episode: 90: Time: 34.2040638923645 Total Reward: 393.3720930232545 Avg_Loss: 2944.9971916675568\n","Episode: 91: Time: 36.851059913635254 Total Reward: 165.073260073264 Avg_Loss: 3440.8458931446075\n","Episode: 92: Time: 39.28203511238098 Total Reward: -12.808219178082474 Avg_Loss: 4019.4141874313354\n","Episode: 93: Time: 37.34454679489136 Total Reward: -22.631578947368652 Avg_Loss: 6162.481150865555\n","Episode: 94: Time: 37.71437692642212 Total Reward: 373.085106382979 Avg_Loss: 1797.745744228363\n","Episode: 95: Time: 23.226691484451294 Total Reward: -11.738271604936159 Avg_Loss: 1332.2138475179672\n","Episode: 96: Time: 38.13578772544861 Total Reward: 264.1549295774667 Avg_Loss: 3076.07048869133\n","Episode: 97: Time: 36.986088275909424 Total Reward: 306.8126888217497 Avg_Loss: 2814.664536714554\n","Episode: 98: Time: 35.13570499420166 Total Reward: 97.98245614035382 Avg_Loss: 5184.642073631287\n","Episode: 99: Time: 34.294528007507324 Total Reward: 394.5104895104894 Avg_Loss: 3605.1788613796234\n","Validation Mean Reward: -94.999999999999 Validation Std Reward: 3.7777003166898047e-14\n","Episode: 100: Time: 38.708064794540405 Total Reward: 549.5993031358776 Avg_Loss: 2797.9641659259796\n","Episode: 101: Time: 37.42980909347534 Total Reward: 201.92832764505516 Avg_Loss: 4408.863108158112\n","Episode: 102: Time: 35.20885753631592 Total Reward: 357.70270270270186 Avg_Loss: 3736.9135599136353\n","Episode: 103: Time: 36.90456199645996 Total Reward: 262.6158940397296 Avg_Loss: 3731.185831308365\n","Episode: 104: Time: 34.63876819610596 Total Reward: 53.40989399293662 Avg_Loss: 3061.4139173030853\n","Episode: 105: Time: 38.307716608047485 Total Reward: 352.88732394365684 Avg_Loss: 2680.095752477646\n","Episode: 106: Time: 37.276952266693115 Total Reward: -14.117647058823849 Avg_Loss: 4981.268536329269\n","Episode: 107: Time: 34.90388083457947 Total Reward: 185.4054054054062 Avg_Loss: 2290.1407123804092\n","Episode: 108: Time: 36.90405297279358 Total Reward: 227.9166666666712 Avg_Loss: 3111.2328729629517\n","Episode: 109: Time: 37.84551763534546 Total Reward: 161.55976676385163 Avg_Loss: 4011.9151582717896\n","Episode: 110: Time: 34.726004123687744 Total Reward: 310.8441558441561 Avg_Loss: 2808.9279730319977\n","Episode: 111: Time: 35.99349856376648 Total Reward: -29.482758620690284 Avg_Loss: 5117.152989149094\n","Episode: 112: Time: 35.008352756500244 Total Reward: 97.72727272727346 Avg_Loss: 3756.6690876483917\n","Episode: 113: Time: 37.513413190841675 Total Reward: 138.84615384615626 Avg_Loss: 3879.3694512844086\n","Episode: 114: Time: 37.89718818664551 Total Reward: 139.52768729642037 Avg_Loss: 3412.304606437683\n","Episode: 115: Time: 36.3354549407959 Total Reward: 54.09090909091221 Avg_Loss: 3708.226250886917\n","Episode: 116: Time: 36.70521879196167 Total Reward: 247.0195439739446 Avg_Loss: 3201.331131219864\n","Episode: 117: Time: 34.944509983062744 Total Reward: 134.45205479452162 Avg_Loss: 3660.4866127967834\n","Episode: 118: Time: 37.44786548614502 Total Reward: 108.57142857143245 Avg_Loss: 13037.755757570267\n","Episode: 119: Time: 37.015279054641724 Total Reward: 224.0789473684245 Avg_Loss: 3149.7560620307922\n","Episode: 120: Time: 35.19274306297302 Total Reward: 196.176470588239 Avg_Loss: 2614.1174685955048\n","Episode: 121: Time: 37.327500343322754 Total Reward: 124.74522292993842 Avg_Loss: 3196.4686584472656\n","Episode: 122: Time: 36.62682843208313 Total Reward: 12.744107744107195 Avg_Loss: 3076.6217896938324\n","Episode: 123: Time: 38.24457049369812 Total Reward: 219.46540880503596 Avg_Loss: 3150.8705493211746\n","Episode: 124: Time: 37.228662729263306 Total Reward: 157.52525252525317 Avg_Loss: 3005.0375249385834\n","Episode: 125: Time: 35.55951690673828 Total Reward: 106.83486238532136 Avg_Loss: 4032.400965690613\n","Episode: 126: Time: 34.71407437324524 Total Reward: 287.165605095536 Avg_Loss: 2992.1020822525024\n","Episode: 127: Time: 37.589030265808105 Total Reward: 251.53465346534782 Avg_Loss: 2556.274245262146\n","Episode: 128: Time: 34.7681930065155 Total Reward: 301.49122807017284 Avg_Loss: 3752.850424051285\n","Episode: 129: Time: 37.27855825424194 Total Reward: -17.261484098940112 Avg_Loss: 4147.349265098572\n","Episode: 130: Time: 37.6243052482605 Total Reward: 9.918032786884357 Avg_Loss: 40278.600358486176\n","Episode: 131: Time: 39.048397064208984 Total Reward: 26.019108280254 Avg_Loss: 7480.368179321289\n","Episode: 132: Time: 34.92643857002258 Total Reward: 3.9399293286220303 Avg_Loss: 5672.367356300354\n","Episode: 133: Time: 35.90154433250427 Total Reward: 368.6678200692031 Avg_Loss: 5405.981603145599\n","Episode: 134: Time: 34.34213900566101 Total Reward: 436.83520599249846 Avg_Loss: 3987.1728699207306\n","Episode: 135: Time: 36.39060950279236 Total Reward: 345.94488188975714 Avg_Loss: 4069.8788821697235\n","Episode: 136: Time: 37.56725335121155 Total Reward: 44.4422310756996 Avg_Loss: 4353.822813510895\n","Episode: 137: Time: 37.81157088279724 Total Reward: 255.6097560975624 Avg_Loss: 4782.920207500458\n","Episode: 138: Time: 36.3822455406189 Total Reward: 95.18404907975727 Avg_Loss: 4464.886867761612\n","Episode: 139: Time: 36.798540353775024 Total Reward: 157.77777777777936 Avg_Loss: 4591.864696979523\n","Episode: 140: Time: 37.546480894088745 Total Reward: 11.89655172413685 Avg_Loss: 4407.159388303757\n","Episode: 141: Time: 36.63931751251221 Total Reward: 195.00000000000094 Avg_Loss: 4074.745363712311\n","Episode: 142: Time: 35.305850982666016 Total Reward: 304.9999999999972 Avg_Loss: 3459.7123878002167\n","Episode: 143: Time: 37.745919942855835 Total Reward: 284.6610169491512 Avg_Loss: 3650.6876740455627\n","Episode: 144: Time: 37.13991928100586 Total Reward: 421.3934426229407 Avg_Loss: 3476.417910337448\n","Episode: 145: Time: 36.83051919937134 Total Reward: 222.64705882353178 Avg_Loss: 4196.008558750153\n","Episode: 146: Time: 36.72729730606079 Total Reward: 331.28205128204075 Avg_Loss: 3821.720279932022\n","Episode: 147: Time: 34.990522146224976 Total Reward: 371.6666666666594 Avg_Loss: 3272.1200017929077\n","Episode: 148: Time: 38.743277072906494 Total Reward: 466.8374558303873 Avg_Loss: 3264.9360077381134\n","Episode: 149: Time: 35.21218466758728 Total Reward: 480.9999999999924 Avg_Loss: 3662.7866480350494\n","Validation Mean Reward: 446.40935142275555 Validation Std Reward: 344.0079508146407\n","Episode: 150: Time: 37.57834982872009 Total Reward: 306.51515151514695 Avg_Loss: 3885.610045194626\n","Episode: 151: Time: 38.42421293258667 Total Reward: 74.0962099125363 Avg_Loss: 6323.608081579208\n","Episode: 152: Time: 37.2779586315155 Total Reward: 14.89010989011095 Avg_Loss: 3865.8322484493256\n","Episode: 153: Time: 36.15162992477417 Total Reward: 160.17241379310713 Avg_Loss: 3757.334452152252\n","Episode: 154: Time: 37.98589253425598 Total Reward: 336.50684931506174 Avg_Loss: 3286.9787979125977\n","Episode: 155: Time: 37.88614463806152 Total Reward: 345.2985074626812 Avg_Loss: 3736.322709083557\n","Episode: 156: Time: 35.400837659835815 Total Reward: 249.61538461538765 Avg_Loss: 3913.606376886368\n","Episode: 157: Time: 36.38124704360962 Total Reward: 266.7021276595784 Avg_Loss: 4166.509275436401\n","Episode: 158: Time: 37.60996437072754 Total Reward: 199.11764705882806 Avg_Loss: 14688.503029823303\n","Episode: 159: Time: 36.20392060279846 Total Reward: 157.59515570934667 Avg_Loss: 9146.443141460419\n","Episode: 160: Time: 36.73467683792114 Total Reward: 63.78378378378643 Avg_Loss: 1951.1611046791077\n","Episode: 161: Time: 38.56062602996826 Total Reward: 90.81081081081089 Avg_Loss: 2579.9733659029007\n","Episode: 162: Time: 36.3406617641449 Total Reward: 478.3333333333246 Avg_Loss: 2809.7292578220367\n","Episode: 163: Time: 39.23968529701233 Total Reward: 368.66782006919857 Avg_Loss: 3779.434668779373\n","Episode: 164: Time: 36.46425819396973 Total Reward: 154.23076923077207 Avg_Loss: 3513.0697782039642\n","Episode: 165: Time: 36.079978704452515 Total Reward: 344.25233644859213 Avg_Loss: 3395.9895956516266\n","Episode: 166: Time: 37.398539304733276 Total Reward: 365.7508532423165 Avg_Loss: 3795.5633642673492\n","Episode: 167: Time: 38.4115846157074 Total Reward: 249.48160535117268 Avg_Loss: 3565.4887092113495\n","Episode: 168: Time: 36.577576637268066 Total Reward: 387.87671232875726 Avg_Loss: 3716.886925458908\n","Episode: 169: Time: 37.9276020526886 Total Reward: 583.861788617879 Avg_Loss: 6107.36149430275\n","Episode: 170: Time: 38.782429456710815 Total Reward: 20.727002967358573 Avg_Loss: 3017.4086258411407\n","Episode: 171: Time: 38.33973479270935 Total Reward: 258.57142857142503 Avg_Loss: 3775.4059545993805\n","Episode: 172: Time: 36.294437408447266 Total Reward: 218.65313653136917 Avg_Loss: 4007.312640428543\n","Episode: 173: Time: 37.2789740562439 Total Reward: 285.9523809523765 Avg_Loss: 4087.101794719696\n","Episode: 174: Time: 36.25971078872681 Total Reward: 268.2958801498131 Avg_Loss: 3418.838441133499\n","Episode: 175: Time: 39.188438177108765 Total Reward: 154.11032028470197 Avg_Loss: 3701.316707611084\n","Episode: 176: Time: 38.03648829460144 Total Reward: 148.54243542435847 Avg_Loss: 3331.6941146850586\n","Episode: 177: Time: 36.83989095687866 Total Reward: 232.75919732441918 Avg_Loss: 4861.00824546814\n","Episode: 178: Time: 36.72677540779114 Total Reward: 93.11881188118811 Avg_Loss: 3771.4882934093475\n","Episode: 179: Time: 39.67141675949097 Total Reward: 165.18808777429865 Avg_Loss: 3994.186920642853\n","Episode: 180: Time: 37.181787729263306 Total Reward: 387.87671232876284 Avg_Loss: 3448.838506937027\n","Episode: 181: Time: 36.6976420879364 Total Reward: 80.25773195876243 Avg_Loss: 2598.8696405887604\n","Episode: 182: Time: 37.92153549194336 Total Reward: 106.14942528735682 Avg_Loss: 3128.470243692398\n","Episode: 183: Time: 38.660135984420776 Total Reward: 110.76131687243125 Avg_Loss: 7784.280131816864\n","Episode: 184: Time: 37.894315242767334 Total Reward: 222.34317343173763 Avg_Loss: 3709.6372570991516\n","Episode: 185: Time: 36.780219078063965 Total Reward: 236.21019108280626 Avg_Loss: 3680.5861864089966\n","Episode: 186: Time: 38.13507580757141 Total Reward: 265.6060606060624 Avg_Loss: 3240.301794052124\n","Episode: 187: Time: 39.252662658691406 Total Reward: 318.1944444444438 Avg_Loss: 3861.795840024948\n","Episode: 188: Time: 37.443559408187866 Total Reward: 225.42253521127145 Avg_Loss: 3662.7305450439453\n","Episode: 189: Time: 36.666574239730835 Total Reward: 222.68953068592384 Avg_Loss: 3277.129695892334\n","Episode: 190: Time: 38.43172001838684 Total Reward: 220.0684931506889 Avg_Loss: 2940.6139826774597\n","Episode: 191: Time: 39.67904806137085 Total Reward: 269.7798742138389 Avg_Loss: 2848.4684277772903\n","Episode: 192: Time: 36.962913274765015 Total Reward: 284.6992481203001 Avg_Loss: 3319.926443338394\n","Episode: 193: Time: 547.681168794632 Total Reward: 53.606811145510505 Avg_Loss: 3472.512932538986\n","Episode: 194: Time: 37.13082671165466 Total Reward: 208.4482758620727 Avg_Loss: 3383.793595314026\n"]}],"source":["import DQN\n","import utils\n","import torch\n","\n","\n","trainerDQN = DQN.DQN(EnvWrapper(env),\n","                model.Nature_Paper_Conv,\n","                lr = 0.00025,\n","                gamma = 0.99,\n","                buffer_size=100000,\n","                batch_size=32,\n","                loss_fn = \"mse_loss\",\n","                use_wandb = False,\n","                device = 'cpu',\n","                seed = 42,\n","                epsilon_scheduler = utils.exponential_decay(1, 1000,0.1),\n","                save_path = utils.get_save_path(\"DQN\",\"./runs/\"))\n","\n","trainerDQN.train(200,50,30,50,50)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"f6mQbBcG8XH2"},"source":["Please include a plot of the training and validation rewards over the episodes in the report. An additional question to answer is does the loss matter in DQN? Why or why not?\n","\n","We can also draw a animation of the car in one game, the code is provided below"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"tFjdCHGA8XH2","executionInfo":{"status":"error","timestamp":1717881472808,"user_tz":240,"elapsed":163,"user":{"displayName":"Anthony Guerrera","userId":"15652414766004964920"}},"outputId":"f7f63f86-4e4c-451f-ee32-ea843b787a3d","colab":{"base_uri":"https://localhost:8080/","height":207}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'gym' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-f88652218445>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CarRacing-v2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontinuous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0meval_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtotal_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainerDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0manim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manimate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"]}],"source":["eval_env = gym.make('CarRacing-v2', continuous=True, render_mode='rgb_array')\n","eval_env = EnvWrapper(eval_env)\n","\n","total_rewards, frames = trainerDQN.play_episode(0,True,42)\n","anim = animate(frames)\n","HTML(anim.to_jshtml())"]},{"cell_type":"markdown","metadata":{"id":"ObB2hsXt8XH3"},"source":["### Double DQN\n","In the original paper, where the algorithim is shown above, the estimated target Q value was computed using the current Q network's weights. However, this can lead to overestimation of the Q values. To mitigate this, we can use the target network to compute the target Q value. This is known as Double DQN.\n","#### Hard updating Target Network (5 points)\n","Original implementations for this involved hard updates, where the model weights were copied to the target network every C steps. This is known as hard updating. This was what was used in the Nature Paper by Mnih et al 2015 \"Human-level control through deep reinforcement learning\"\n","\n","Please implement this by implementing the `_optimize_model` and `_update_model` classes in `HardUpdateDQN` in `DQN.py`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fwh6QXB58XH3","outputId":"529bd364-0c26-4145-aa7e-ef5387413644"},"outputs":[{"ename":"NameError","evalue":"name 'DQN' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-be6f24b32a13>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m trainerHardUpdateDQN = DQN.HardUpdateDQN(EnvWrapper(env),\n\u001b[0m\u001b[1;32m      2\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNature_Paper_Conv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0mupdate_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.00025\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'DQN' is not defined"]}],"source":["trainerHardUpdateDQN = DQN.HardUpdateDQN(EnvWrapper(env),\n","                model.Nature_Paper_Conv,\n","                update_freq = 100,\n","                lr = 0.00025,\n","                gamma = 0.99,\n","                buffer_size=100000,\n","                batch_size=32,\n","                loss_fn = \"mse_loss\",\n","                use_wandb = False,\n","                device = 'cuda',\n","                seed = 42,\n","                epsilon_scheduler = utils.exponential_decay(1, 1000,0.1),\n","                save_path = utils.get_save_path(\"DoubleDQN_HardUpdates/\",\"./runs/\"))\n","\n","trainerHardUpdateDQN.train(200,50,30,50,50)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"27v08En88XH4","outputId":"4e5bc545-16d1-4a87-d3aa-3cbe2b3c77bd"},"outputs":[{"ename":"NameError","evalue":"name 'trainerHardUpdateDQN' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-19e778623f56>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtotal_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainerHardUpdateDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0manim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manimate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_jshtml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'trainerHardUpdateDQN' is not defined"]}],"source":["total_rewards, frames = trainerHardUpdateDQN.play_episode(0,True,42)\n","anim = animate(frames)\n","HTML(anim.to_jshtml())"]},{"cell_type":"markdown","metadata":{"id":"RF-Ew-Lz8XH4"},"source":["#### Soft Updates (5 points)\n","A more recent improvement is to use soft updates, also known as Polyak averaging, where the target network is updated with a small fraction of the current model weights every step. In other words:\n","$$\\theta_{target} = \\tau \\theta_{model} + (1-\\tau) \\theta_{target}$$\n","for some $\\tau << 1$\n","Please implement this by implementing the `_update_model` class in `SoftUpdateDQN` in `DQN.py`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1JpUI1td8XH4","outputId":"e7069e09-150b-4b25-e1fd-972b1248e345"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: './runs/DoubleDQN_SoftUpdates'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m traineSoftUpdateDQN \u001b[38;5;241m=\u001b[39m DQN\u001b[38;5;241m.\u001b[39mSoftUpdateDQN(EnvWrapper(env),\n\u001b[1;32m      2\u001b[0m                 model\u001b[38;5;241m.\u001b[39mNature_Paper_Conv,\n\u001b[1;32m      3\u001b[0m                 tau \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m      4\u001b[0m                 update_freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      5\u001b[0m                 lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.00025\u001b[39m,\n\u001b[1;32m      6\u001b[0m                 gamma \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.99\u001b[39m,\n\u001b[1;32m      7\u001b[0m                 buffer_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m,\n\u001b[1;32m      8\u001b[0m                 batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m      9\u001b[0m                 loss_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m                 use_wandb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m                 device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m                 seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m     13\u001b[0m                 epsilon_scheduler \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mexponential_decay(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1000\u001b[39m,\u001b[38;5;241m0.1\u001b[39m),\n\u001b[0;32m---> 14\u001b[0m                 save_path \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_save_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDoubleDQN_SoftUpdates\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./runs/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m traineSoftUpdateDQN\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m200\u001b[39m,\u001b[38;5;241m50\u001b[39m,\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m50\u001b[39m,\u001b[38;5;241m50\u001b[39m)\n","File \u001b[0;32m/mnt/SSD3/lawrence/ECE239-Project4/utils.py:44\u001b[0m, in \u001b[0;36mget_save_path\u001b[0;34m(suffix, directory)\u001b[0m\n\u001b[1;32m     42\u001b[0m save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory,suffix)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#find the number of run directories in the directory\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m runs \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m d]\n\u001b[1;32m     45\u001b[0m runs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(runs,key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mint\u001b[39m(x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(runs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './runs/DoubleDQN_SoftUpdates'"]}],"source":["traineSoftUpdateDQN = DQN.SoftUpdateDQN(EnvWrapper(env),\n","                model.Nature_Paper_Conv,\n","                tau = 0.01,\n","                update_freq = 1,\n","                lr = 0.00025,\n","                gamma = 0.99,\n","                buffer_size=100000,\n","                batch_size=32,\n","                loss_fn = \"mse_loss\",\n","                use_wandb = False,\n","                device = 'cuda',\n","                seed = 42,\n","                epsilon_scheduler = utils.exponential_decay(1, 1000,0.1),\n","                save_path = utils.get_save_path(\"DoubleDQN_SoftUpdates\",\"./runs/\"))\n","\n","traineSoftUpdateDQN.train(200,50,30,50,50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"50aeI93r8XH5"},"outputs":[],"source":["total_rewards, frames = traineSoftUpdateDQN.play_episode(0,True,42)\n","anim = animate(frames)\n","HTML(anim.to_jshtml())"]},{"cell_type":"markdown","metadata":{"id":"mHY4t6qW8XH5"},"source":["#### Questions:\n","- Which method performed better? (5 points)\n","- If we modify the $\\tau$ for soft updates or the $C$ for the hard updates, how does this affect the performance of the model, come up with a intuition for this, then experimentally verify this.\n"," (5 points)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}